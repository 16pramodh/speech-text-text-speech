{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"!nvidia-smi\n!nvcc --version\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:52:52.499692Z","iopub.execute_input":"2025-08-29T06:52:52.500619Z","iopub.status.idle":"2025-08-29T06:52:53.066436Z","shell.execute_reply.started":"2025-08-29T06:52:52.500583Z","shell.execute_reply":"2025-08-29T06:52:53.065469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#if you see this it worked!!!\n\n!add-apt-repository -y ppa:ubuntuhandbook1/ffmpeg8\n!apt-get update -y\n!apt-get install -y ffmpeg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ffmpeg -version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio transformers peft datasets\n!pip cache purge\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:25:33.309474Z","iopub.execute_input":"2025-08-29T06:25:33.310034Z","iopub.status.idle":"2025-08-29T06:25:54.874327Z","shell.execute_reply.started":"2025-08-29T06:25:33.309997Z","shell.execute_reply":"2025-08-29T06:25:54.873604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:26:00.581601Z","iopub.execute_input":"2025-08-29T06:26:00.582534Z","iopub.status.idle":"2025-08-29T06:29:03.293753Z","shell.execute_reply.started":"2025-08-29T06:26:00.582501Z","shell.execute_reply":"2025-08-29T06:29:03.293062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade transformers==4.55.4 peft==0.17.1 \"datasets[audio]>=2.21.0\" accelerate evaluate jiwer tensorboard\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:29:12.620207Z","iopub.execute_input":"2025-08-29T06:29:12.620493Z","iopub.status.idle":"2025-08-29T06:29:28.808225Z","shell.execute_reply.started":"2025-08-29T06:29:12.620463Z","shell.execute_reply":"2025-08-29T06:29:28.807358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#NOTE: WHATEVER HAPPENS, MAKE SURE datasets IS NOT 4.0.0\n\n!pip uninstall -y datasets torchcodec soundfile\n!pip install \"datasets[audio]==2.21.0\" --no-cache-dir\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:29:57.156626Z","iopub.execute_input":"2025-08-29T06:29:57.156923Z","iopub.status.idle":"2025-08-29T06:30:04.548973Z","shell.execute_reply.started":"2025-08-29T06:29:57.156896Z","shell.execute_reply":"2025-08-29T06:30:04.548203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom peft import prepare_model_for_kbit_training\n\nfrom datasets import load_dataset\n\nimport torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\nimport evaluate\n\nfrom transformers import Seq2SeqTrainingArguments\n\nfrom transformers import Seq2SeqTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:31:01.044733Z","iopub.execute_input":"2025-08-29T06:31:01.045104Z","iopub.status.idle":"2025-08-29T06:31:21.653866Z","shell.execute_reply.started":"2025-08-29T06:31:01.045073Z","shell.execute_reply":"2025-08-29T06:31:21.653080Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# loading dataset","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\n# Get the value of the secret you created\nHF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")\n\n# Now you can use this token for authentication\nfrom huggingface_hub import login\nlogin(token=HF_TOKEN)\n\n# You can now access gated models or push models to the Hugging Face Hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:31:27.996118Z","iopub.execute_input":"2025-08-29T06:31:27.997074Z","iopub.status.idle":"2025-08-29T06:31:28.232367Z","shell.execute_reply.started":"2025-08-29T06:31:27.997047Z","shell.execute_reply":"2025-08-29T06:31:28.231824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Audio\n\n# Load the dataset, specifying the 'test' split since it's the only one available\n# Use 'token=True' to automatically use the token you've logged in with\n# If you didn't run 'huggingface-cli login', you can pass your token here: token=\"hf_...\"\nsvarah_dataset = load_dataset(\"ai4bharat/Svarah\", split=\"test\", token=True)\n\nsvarah_dataset = svarah_dataset.cast_column('audio_filepath', Audio(decode=True))\n\n# Split the dataset into a training and validation set\n# We'll use 15% of the data for validation and the remaining 85% for training\n# Setting seed for reproducibility\nsplit_dataset = svarah_dataset.train_test_split(test_size=0.15, seed=42)\n\n# Access the new splits\ntrain_dataset = split_dataset['train']\nvalidation_dataset = split_dataset['test']  # The 'test_size' data is named 'test' by this method\n\n# Print the sizes to confirm the split\nprint(f\"Total dataset size: {len(svarah_dataset)}\")\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(validation_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:31:35.742320Z","iopub.execute_input":"2025-08-29T06:31:35.743118Z","iopub.status.idle":"2025-08-29T06:31:48.624591Z","shell.execute_reply.started":"2025-08-29T06:31:35.743087Z","shell.execute_reply":"2025-08-29T06:31:48.624035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this is to test and check if array is being returned or not\n\nimport numpy as np\n\nprint(svarah_dataset.column_names)\nprint(svarah_dataset[0])\n\ntrain_dataset = train_dataset.cast_column(\"audio_filepath\", Audio(decode=True))\n\n# now try accessing a row\nsample = train_dataset[0][\"audio_filepath\"]\n\nprint(type(sample))   # should show ['path', 'array', 'sampling_rate']\nprint(sample[\"array\"], sample[\"sampling_rate\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:32:07.734265Z","iopub.execute_input":"2025-08-29T06:32:07.734543Z","iopub.status.idle":"2025-08-29T06:32:21.365487Z","shell.execute_reply.started":"2025-08-29T06:32:07.734523Z","shell.execute_reply":"2025-08-29T06:32:21.364668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Whisper model and processor","metadata":{}},{"cell_type":"code","source":"# load model and processor\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small.en\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small.en\")\n\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:32:28.475644Z","iopub.execute_input":"2025-08-29T06:32:28.476853Z","iopub.status.idle":"2025-08-29T06:32:37.840793Z","shell.execute_reply.started":"2025-08-29T06:32:28.476809Z","shell.execute_reply":"2025-08-29T06:32:37.840266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Applying LORA on model","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n)\n\n# Apply the PEFT configuration to the model\nmodel = get_peft_model(model, lora_config)\n\n# Print the number of trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:32:44.094430Z","iopub.execute_input":"2025-08-29T06:32:44.094725Z","iopub.status.idle":"2025-08-29T06:32:44.216003Z","shell.execute_reply.started":"2025-08-29T06:32:44.094704Z","shell.execute_reply":"2025-08-29T06:32:44.215336Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing of dataset","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef prepare_dataset(batch):\n    audio = batch[\"audio_filepath\"]  # Access underlying file path if available\n    processed_output = processor(\n        audio=audio['array'],\n        sampling_rate=audio['sampling_rate'],\n        text=batch[\"text\"],\n    )\n    batch[\"input_features\"] = processed_output.input_features[0]\n    batch[\"labels\"] = processed_output.labels[0]\n    batch[\"attention_mask\"] = processed_output.attention_mask[0]\n    return batch\n\n\n# Run preprocessing (only on small test subset first, then full)\ntds = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)\nvds = validation_dataset.map(prepare_dataset, remove_columns=validation_dataset.column_names, num_proc=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:59:20.379515Z","iopub.execute_input":"2025-08-29T06:59:20.379873Z","iopub.status.idle":"2025-08-29T07:00:05.334131Z","shell.execute_reply.started":"2025-08-29T06:59:20.379846Z","shell.execute_reply":"2025-08-29T07:00:05.332975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tds  #just to see how it is","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:34:40.657450Z","iopub.execute_input":"2025-08-29T06:34:40.657749Z","iopub.status.idle":"2025-08-29T06:34:40.663599Z","shell.execute_reply.started":"2025-08-29T06:34:40.657722Z","shell.execute_reply":"2025-08-29T06:34:40.662821Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data collator","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if labels.dim() > 1 and (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n        elif labels.dim() == 1 and labels[0] == self.decoder_start_token_id:\n            labels = labels[1:].unsqueeze(0)\n\n\n        batch[\"labels\"] = labels\n\n        return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:34:48.215230Z","iopub.execute_input":"2025-08-29T06:34:48.215489Z","iopub.status.idle":"2025-08-29T06:34:48.223124Z","shell.execute_reply.started":"2025-08-29T06:34:48.215472Z","shell.execute_reply":"2025-08-29T06:34:48.222264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:34:58.231407Z","iopub.execute_input":"2025-08-29T06:34:58.232139Z","iopub.status.idle":"2025-08-29T06:34:58.236455Z","shell.execute_reply.started":"2025-08-29T06:34:58.232106Z","shell.execute_reply":"2025-08-29T06:34:58.235752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"metric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    # we do not want to group tokens when computing the metrics\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:35:01.573732Z","iopub.execute_input":"2025-08-29T06:35:01.574049Z","iopub.status.idle":"2025-08-29T06:35:02.241640Z","shell.execute_reply.started":"2025-08-29T06:35:01.574029Z","shell.execute_reply":"2025-08-29T06:35:02.240939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating Trainer","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-small-en\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=1e-6,\n    max_steps=10000,              # ðŸ‘ˆ preferred over num_train_epochs\n    logging_steps=100,\n    save_steps=500,\n    eval_steps=500,\n    do_train=True,\n    do_eval=True,                 # old-style eval trigger\n    predict_with_generate=True,\n    save_total_limit=2,\n    report_to=\"none\", #this is for disabling wandB\n    fp16=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:38:42.882913Z","iopub.execute_input":"2025-08-29T06:38:42.883689Z","iopub.status.idle":"2025-08-29T06:38:42.916996Z","shell.execute_reply.started":"2025-08-29T06:38:42.883663Z","shell.execute_reply":"2025-08-29T06:38:42.916442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=tds,\n    eval_dataset=vds,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:38:46.784442Z","iopub.execute_input":"2025-08-29T06:38:46.785005Z","iopub.status.idle":"2025-08-29T06:38:46.806060Z","shell.execute_reply.started":"2025-08-29T06:38:46.784971Z","shell.execute_reply":"2025-08-29T06:38:46.805406Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.cuda.device_count())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:44:59.269731Z","iopub.execute_input":"2025-08-29T06:44:59.270100Z","iopub.status.idle":"2025-08-29T06:44:59.274772Z","shell.execute_reply.started":"2025-08-29T06:44:59.270078Z","shell.execute_reply":"2025-08-29T06:44:59.274034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# os.environ[\"WANDB_DISABLED\"] = \"true\"  #wandB is used for checking metrics and all, which we don't necessarily need here, without an api key you can't run this\n\n# trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-29T06:38:50.968304Z","iopub.execute_input":"2025-08-29T06:38:50.968588Z","iopub.status.idle":"2025-08-29T06:38:57.113312Z","shell.execute_reply.started":"2025-08-29T06:38:50.968567Z","shell.execute_reply":"2025-08-29T06:38:57.112229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.save_model(\"./whisper-small-en-final\")\n# tokenizer.save_pretrained(\"./whisper-small-en-final\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !zip -r whisper-small-en-final.zip ./whisper-small-en-final","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}